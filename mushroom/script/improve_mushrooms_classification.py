# -*- coding: utf-8 -*-
"""Improve-mushrooms classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKlWrT3YULJ-wCk4f5TBlHaJl7pQDYuq

#First attempt

## Import the data
"""

import pandas as pd
import numpy as np

# reading
url="https://drive.google.com/file/d/1ljJfs1Rue1PRouBeZVl3DabqWRrfI8ZL/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)
data=df.copy()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder,PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error,r2_score
from scipy.sparse import csr_matrix
from scipy import sparse
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

X = df.drop(columns="poisonous")
y = df["poisonous"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

"""## Data Exploration"""

X_train.head()

X_train.info()

"""The only numeric columnn of our train set is "Id". Our column trandformer below will ignore it.

## Preprocessing the data
"""

cat_col=X_train.select_dtypes(exclude = 'number').copy().columns # Extracting the names of columns
categoric_pipe = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="N_A"),       # Our data set X_train has no missing value, but it might not be the case for  X_test
    OneHotEncoder(handle_unknown="ignore")
)  

numeric_pipe = make_pipeline(                                 #useful if we have meaningful nuemric columns
    SimpleImputer(strategy="mean"))

preprocessor = ColumnTransformer(transformers=[
    ('category', categoric_pipe, cat_col)
    #('number', numeric_pipe, num_col) # We ignore the numeric column
])

dt_pipeline = make_pipeline(preprocessor, 
                            #StandardScaler(with_mean=False) # no need to scale the onHotEncoded data
                           )

X_train_encoded=dt_pipeline.fit_transform(X_train)
X_train_encoded=pd.DataFrame(X_train_encoded.todense())


X_test_encoded=dt_pipeline.transform(X_test)
X_test_encoded=pd.DataFrame(X_test_encoded.todense())

"""## Neural network with 5 layers:
We have a binary classification problem, and therefore set the last activation to be the sigmoid function $f(x)=\frac{1}{1+\exp(-x)}$.
"""

model = Sequential(
    [
        tf.keras.Input(shape=(42,)),
        Dense(units=30, activation='linear', name = 'layer1'),
        Dense(20, activation='relu', name = 'layer2'),
        Dense(5, activation='relu', name = 'layer3'),
        Dense(3, activation='linear', name = 'layer4'),
        Dense(1, activation='sigmoid', name = 'layer5')
     ]
)

model.summary()

"""The binary cross entropy loss function $-y\log(\hat{y} )-(1-y)\log(1-\hat{y})$ works pretty well with binary classification problems. The optimizer Adam is faster."""

model.compile(                                          #compiling the model
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
)

model.fit(                                              #fitting the model
    X_train_encoded,y_train,                            
    epochs=100,
)

"""##Predictions of the model
The model is now ready to make predictions. 
"""

predictions_train_set = model.predict(X_train_encoded)    #predictions for the train set
predictions_test_set = model.predict(X_test_encoded)      #prediction for the test set

"""Recall that the output of our model is a vector of real numbers, each representing the probability that a given mushroom (describe by the correcponding row in the data set) is poisonous. We set our decisive treshold to be equal to $0.1$: We classify a mushroom as poisonous if, and only if our model predicts that its probability to be poisonous is strictly greater than $0.1$. """

treshold=0.1
yhat_train=list((pd.DataFrame(predictions_train_set).iloc[:,0]>treshold).astype(int))
yhat_test=list((pd.DataFrame(predictions_test_set).iloc[:,0]>treshold).astype(int))

"""Overview of the predictions."""

[(yhat_test[i],list(y_test)[i]) for i in range(20)]

"""## Model performence

We use the confusion matrix to evaluate the performence of our model. We compute: 
- $tn$: The number of true negatives. It corresponds to the number of eatable mushrooms that our model accurately classified.
- $fp$: The number of false positives. It corresponds to the number of eatable mushrooms that our model missclassified.
- $fn$: The number of false negatives. It corresponds to the number of poisonous mushrooms that our model missclassified.
- $tp$: The number of true positives. It corresponds to the number of poisonous mushrooms that our model accurately classified.
"""

from sklearn.metrics import confusion_matrix
#confusion_matrix(list(y_test),list(yhat_test))
tn, fp, fn, tp = confusion_matrix(list(y_test),list(yhat_test)).ravel()
tn, fp, fn, tp

#doind the task above manually
tp_test=sum([list(yhat_test)[i]*list(y_test)[i] for i in range(len(y_test))])
fp_test=sum([yhat_test[i]*(1-list(y_test)[i]) for i in range(len(y_test))])
tn_test=sum([(1-yhat_test[i])*(1-list(y_test)[i]) for i in range(len(y_test))])
fn_test=sum([(1-yhat_test[i])*list(y_test)[i] for i in range(len(y_test))])
tn_test,fp_test,fn_test,tp_test

"""## Prepare the submission"""

# reading
url="https://drive.google.com/file/d/1rHAgVfd7vtZv3bj4Fb0MqS5PcRwOLC5I/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df_testing = pd.read_csv(path)

data_testing=df_testing.copy()

X_testing_encoded=dt_pipeline.transform(data_testing)
X_testing_encoded=pd.DataFrame(X_testing_encoded.todense())
predictions_testing = model.predict(X_testing_encoded)
yhat_testing=list((pd.DataFrame(predictions_testing).iloc[:,0]>treshold).astype(int))

data_testing["poisonous"]=yhat_testing
result=data_testing[["Id","poisonous"]]
result.to_csv("attemp_x_Gauss.csv",index=False)

"""#Improvement?"""

import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder,PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error,r2_score
from scipy.sparse import csr_matrix
from scipy import sparse
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# reading
url="https://drive.google.com/file/d/1ljJfs1Rue1PRouBeZVl3DabqWRrfI8ZL/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)
data=df.copy()

X = df.drop(columns="poisonous")
y = df["poisonous"]
X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.4, random_state=123)
X_test, X_val, y_test, y_val = train_test_split(X_, y_, test_size=0.5, random_state=123)

X_train.head()

X_train.info()

#Preprocessing data
cat_col=X_train.select_dtypes(exclude = 'number').copy().columns # Extracting the names of columns
categoric_pipe = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="N_A"),       # Our data set X_train has no missing value, but it might not be the case for  X_test
    OneHotEncoder(handle_unknown="ignore")
)  

numeric_pipe = make_pipeline(                                 #useful if we have meaningful nuemric columns
    SimpleImputer(strategy="mean"))

preprocessor = ColumnTransformer(transformers=[
    ('category', categoric_pipe, cat_col)
    #('number', numeric_pipe, num_col) # We ignore the numeric column
])

dt_pipeline = make_pipeline(preprocessor, 
                            #PolynomialFeatures(degree=2,interaction_only=False, include_bias=True),
                            #StandardScaler(with_mean=False) # no need to scale the onHotEncoded data
                           )


X_train_encoded=dt_pipeline.fit_transform(X_train)
X_train_encoded=pd.DataFrame(X_train_encoded.todense())


X_test_encoded=dt_pipeline.transform(X_test)
X_test_encoded=pd.DataFrame(X_test_encoded.todense())

X_val_encoded=dt_pipeline.transform(X_val)
X_val_encoded=pd.DataFrame(X_val_encoded.todense())

X_train_encoded.shape

"""## Neural network with 5 layers:
We have a binary classification problem, and therefore set the last activation to be the sigmoid function $f(x)=\frac{1}{1+\exp(-x)}$.

"""

model = Sequential(
    [
        tf.keras.Input(shape=(X_train_encoded.shape[1],)),
        Dense(units=45, activation='linear', name = 'layer0'),
     
        Dense(units=30, activation='sigmoid', name = 'layer1'),
        Dense(20, activation='relu', name = 'layer2'),
        Dense(5, activation='relu', name = 'layer3'),
        Dense(3, activation='linear', name = 'layer4'),
        Dense(1, activation='sigmoid', name = 'layer5')
     ]
)

model.summary()

"""The binary cross entropy loss function $-y\log(\hat{y} )-(1-y)\log(1-\hat{y})$ works pretty well with binary classification problems. The optimizer Adam is faster."""

model.compile(                                          #compiling the model
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),
     metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()]
)

model.fit(                                              #fitting the model
    X_train_encoded,y_train,                            
    epochs=100,
)

"""
##Predictions of the model
The model is now ready to make predictions. """

predictions_train_set = model.predict(X_train_encoded)    #predictions for the train set
predictions_val_set = model.predict(X_val_encoded)      #prediction for the val set
predictions_test_set = model.predict(X_test_encoded)      #prediction for the test set

"""Recall that the output of our model is a vector of real numbers, each representing the probability that a given mushroom (describe by the correcponding row in the data set) is poisonous. We set our decisive treshold to be equal to $0.25$: We classify a mushroom as poisonous if, and only if our model predicts that its probability to be poisonous is strictly greater than $0.25$. """

treshold=0.25
yhat_train=list((pd.DataFrame(predictions_train_set).iloc[:,0]>treshold).astype(int))
yhat_test=list((pd.DataFrame(predictions_test_set).iloc[:,0]>treshold).astype(int))
yhat_val=list((pd.DataFrame(predictions_val_set).iloc[:,0]>treshold).astype(int))

"""
Overview of the predictions."""

[(yhat_test[i],list(y_test)[i]) for i in range(20)]

"""

## Model performence

We use the confusion matrix to evaluate the performence of our model. We compute: 
- $tn$: The number of true negatives. It corresponds to the number of eatable mushrooms that our model accurately classified.
- $fp$: The number of false positives. It corresponds to the number of eatable mushrooms that our model missclassified.
- $fn$: The number of false negatives. It corresponds to the number of poisonous mushrooms that our model missclassified.
- $tp$: The number of true positives. It corresponds to the number of poisonous mushrooms that our model accurately classified."""

from sklearn.metrics import confusion_matrix
#confusion_matrix(list(y_test),list(yhat_test))
tn_test, fp_test, fn_test, tp_test = confusion_matrix(list(y_test),list(yhat_test)).ravel()
tn_val, fp_val, fn_val, tp_val = confusion_matrix(list(y_val),list(yhat_val)).ravel()
fp_test+fn_test, fp_val+fn_val



"""##Best treshold

"""

def acc_treshol(y_1,y__1,y_2,y__2,y_3,y__3, ep):
  y_1_hat=list((pd.DataFrame(y_1).iloc[:,0]>ep).astype(int))
  y_2_hat=list((pd.DataFrame(y_2).iloc[:,0]>ep).astype(int))
  y_3_hat=list((pd.DataFrame(y_3).iloc[:,0]>ep).astype(int))

  tn_1, fp_1, fn_1, tp_1 = confusion_matrix(list(y__1),list(y_1_hat)).ravel()
  tn_2, fp_2, fn_2, tp_2 = confusion_matrix(list(y__2),list(y_2_hat)).ravel()
  tn_3, fp_3, fn_3, tp_3 = confusion_matrix(list(y__3),list(y_3_hat)).ravel()
  return fp_1+fn_1,fp_2+fn_2,fp_3+fn_3

acc_treshol(predictions_train_set,y_train,predictions_test_set,y_test,predictions_val_set,y_val, 0.005)

acc_train=[(100/len(X_train))*acc_treshol(predictions_train_set,y_train,predictions_test_set,y_test,predictions_val_set,y_val, i/100)[0] for i in range(1,100)]
acc_test=[(100/len(X_test))*acc_treshol(predictions_train_set,y_train,predictions_test_set,y_test,predictions_val_set,y_val, i/100)[1] for i in range(1,100)]
acc_val=[(100/len(X_val))*acc_treshol(predictions_train_set,y_train,predictions_test_set,y_test,predictions_val_set,y_val, i/100)[2] for i in range(1,100)]

plt.plot(range(1,60), acc_train[1:60], color='green', marker='o', linestyle='dashed', linewidth=1, markersize=2)
plt.plot(range(1,60), acc_test[1:60], color='red', marker='o', linestyle='dashed', linewidth=1, markersize=2)
plt.plot(range(1,60), acc_test[1:60], color='blue', marker='o', linestyle='dashed', linewidth=1, markersize=2)

acc_treshol(predictions_train_set,y_train,predictions_test_set,y_test,predictions_val_set,y_val, 0.4)

"""Not very happy with the new model.  """

treshold=0.4
yhat_test=list((pd.DataFrame(predictions_test_set).iloc[:,0]>treshold).astype(int))
tn_test, fp_test, fn_test, tp_test = confusion_matrix(list(y_test),list(yhat_test)).ravel()
tn_test, fp_test, fn_test, tp_test

"""In the next attempt we will encode the data differently and create additional features."""

