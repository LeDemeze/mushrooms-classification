# -*- coding: utf-8 -*-
"""mushrooms classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecCnHSrCvpjK5chQZhckliAU7FkhwY1O

#First attempt

## Import the data
"""

import pandas as pd
import numpy as np

# reading
url="https://drive.google.com/file/d/1ljJfs1Rue1PRouBeZVl3DabqWRrfI8ZL/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)
data=df.copy()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder,PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error,r2_score
from scipy.sparse import csr_matrix
from scipy import sparse
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

X = df.drop(columns="poisonous")
y = df["poisonous"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

"""## Data Exploration"""

X_train.head()

X_train.info()

"""The only numeric columnn of our train set is "Id". Our column trandformer below will ignore it.

## Preprocessing the data
"""

cat_col=X_train.select_dtypes(exclude = 'number').copy().columns # Extracting the names of columns
categoric_pipe = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="N_A"),       # Our data set X_train has no missing value, but it might not be the case for  X_test
    OneHotEncoder(handle_unknown="ignore")
)  

numeric_pipe = make_pipeline(                                 #useful if we have meaningful nuemric columns
    SimpleImputer(strategy="mean"))

preprocessor = ColumnTransformer(transformers=[
    ('category', categoric_pipe, cat_col)
    #('number', numeric_pipe, num_col) # We ignore the numeric column
])

dt_pipeline = make_pipeline(preprocessor, 
                            #StandardScaler(with_mean=False) # no need to scale the onHotEncoded data
                           )

X_train_encoded=dt_pipeline.fit_transform(X_train)
X_train_encoded=pd.DataFrame(X_train_encoded.todense())


X_test_encoded=dt_pipeline.transform(X_test)
X_test_encoded=pd.DataFrame(X_test_encoded.todense())

"""## Neural network with 5 layers:
We have a binary classification problem, and therefore set the last activation to be the sigmoid function $f(x)=\frac{1}{1+\exp(-x)}$.
"""

model = Sequential(
    [
        tf.keras.Input(shape=(42,)),
        Dense(units=30, activation='linear', name = 'layer1'),
        Dense(20, activation='relu', name = 'layer2'),
        Dense(5, activation='relu', name = 'layer3'),
        Dense(3, activation='linear', name = 'layer4'),
        Dense(1, activation='sigmoid', name = 'layer5')
     ]
)

model.summary()

"""The binary cross entropy loss function $-y\log(\hat{y} )-(1-y)\log(1-\hat{y})$ works pretty well with binary classification problems. The optimizer Adam is faster."""

model.compile(                                          #compiling the model
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)

model.fit(                                              #fitting the model
    X_train_encoded,y_train,                            
    epochs=60,
)

"""##Predictions of the model
The model is now ready to make predictions. 
"""

predictions_train_set = model.predict(X_train_encoded)    #predictions for the train set
predictions_test_set = model.predict(X_test_encoded)      #prediction for the test set

"""Recall that the output of our model is a vector of real numbers, each representing the probability that a given mushroom (describe by the correcponding row in the data set) is poisonous. We set our decisive treshold to be equal to $0.25$: We classify a mushroom as poisonous if, and only if our model predicts that its probability to be poisonous is strictly greater than $0.25$. """

treshold=0.25
yhat_train=list((pd.DataFrame(predictions_train_set).iloc[:,0]>treshold).astype(int))
yhat_test=list((pd.DataFrame(predictions_test_set).iloc[:,0]>treshold).astype(int))

"""Overview of the predictions."""

[(yhat_test[i],list(y_test)[i]) for i in range(20)]

"""## Model performence

We use the confusion matrix to evaluate the performence of our model. We compute: 
- $tn$: The number of true negatives. It corresponds to the number of eatable mushrooms that our model accurately classified.
- $fp$: The number of false positives. It corresponds to the number of eatable mushrooms that our model missclassified.
- $fn$: The number of false negatives. It corresponds to the number of poisonous mushrooms that our model missclassified.
- $tp$: The number of true positives. It corresponds to the number of poisonous mushrooms that our model accurately classified.
"""

from sklearn.metrics import confusion_matrix
#confusion_matrix(list(y_test),list(yhat_test))
tn, fp, fn, tp = confusion_matrix(list(y_test),list(yhat_test)).ravel()
tn, fp, fn, tp

#doind the task above manually
tp_test=sum([list(yhat_test)[i]*list(y_test)[i] for i in range(len(y_test))])
fp_test=sum([yhat_test[i]*(1-list(y_test)[i]) for i in range(len(y_test))])
tn_test=sum([(1-yhat_test[i])*(1-list(y_test)[i]) for i in range(len(y_test))])
fn_test=sum([(1-yhat_test[i])*list(y_test)[i] for i in range(len(y_test))])
tn_test,fp_test,fn_test,tp_test

# reading
url="https://drive.google.com/file/d/1rHAgVfd7vtZv3bj4Fb0MqS5PcRwOLC5I/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df_testing = pd.read_csv(path)

data_testing=df_testing.copy()

X_testing_encoded=dt_pipeline.transform(data_testing)
X_testing_encoded=pd.DataFrame(X_testing_encoded.todense())
predictions_testing = model.predict(X_testing_encoded)
yhat_testing=list((pd.DataFrame(predictions_testing).iloc[:,0]>treshold).astype(int))

data_testing["poisonous"]=yhat_testing
result=data_testing[["Id","poisonous"]]
result.to_csv("attemp_2_Gauss.csv",index=False)